{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2a5c13a-e028-4d63-be1d-226a004e41bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "from IPython.display import Image\n",
    "from PIL import Image as PILImage\n",
    "import io\n",
    "import time\n",
    "import sys\n",
    "from unitree_sdk2py.core.channel import ChannelSubscriber, ChannelFactoryInitialize\n",
    "from unitree_sdk2py.idl.default import unitree_go_msg_dds__SportModeState_\n",
    "from unitree_sdk2py.idl.unitree_go.msg.dds_ import SportModeState_\n",
    "from unitree_sdk2py.go2.sport.sport_client import (\n",
    "    SportClient,\n",
    "    PathPoint,\n",
    "    SPORT_PATH_POINT_SIZE,\n",
    ")\n",
    "from unitree_sdk2py.go2.obstacles_avoid.obstacles_avoid_client import ObstaclesAvoidClient\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1945ebd2-c71b-4c7e-9d27-cd889169e7a6",
   "metadata": {},
   "source": [
    "## get camera feed and show live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bafc0d8-0c69-4fca-95cc-b6bb3e58fe1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyrealsense2.pyrealsense2.pipeline_profile at 0xffff4713eef0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "Object_colors = list(np.random.rand(80,3)*255)\n",
    "# Start pipeline\n",
    "pipeline.start(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a047bb41-6126-44ef-8d23-9b43cf9d5f2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Wait for a frame from the RealSense camera\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     frames \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     color_frame \u001b[38;5;241m=\u001b[39m frames\u001b[38;5;241m.\u001b[39mget_color_frame()\n\u001b[1;32m      5\u001b[0m     color_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(color_frame\u001b[38;5;241m.\u001b[39mget_data())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Wait for a frame from the RealSense camera\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    color_frame = frames.get_color_frame()\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # plt.imshow(color_image)\n",
    "    # plt.show()\n",
    "    # # print(color_image.shape)\n",
    "    # time.sleep(0.5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6290b152-7a58-4eb7-98c5-cbba27405dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Configure depth and color streams\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "\n",
    "# Get device product line for setting a supporting resolution\n",
    "pipeline_wrapper = rs.pipeline_wrapper(pipeline)\n",
    "pipeline_profile = config.resolve(pipeline_wrapper)\n",
    "device = pipeline_profile.get_device()\n",
    "device_product_line = str(device.get_info(rs.camera_info.product_line))\n",
    "\n",
    "found_rgb = False\n",
    "for s in device.sensors:\n",
    "    if s.get_info(rs.camera_info.name) == 'RGB Camera':\n",
    "        found_rgb = True\n",
    "        break\n",
    "if not found_rgb:\n",
    "    print(\"The demo requires Depth camera with Color sensor\")\n",
    "    exit(0)\n",
    "\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "\n",
    "# Start streaming\n",
    "pipeline.start(config)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "\n",
    "        # Wait for a coherent pair of frames: depth and color\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        if not depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        # Convert images to numpy arrays\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "        # Apply colormap on depth image (image must be converted to 8-bit per pixel first)\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "\n",
    "        depth_colormap_dim = depth_colormap.shape\n",
    "        color_colormap_dim = color_image.shape\n",
    "\n",
    "        # If depth and color resolutions are different, resize color image to match depth image for display\n",
    "        if depth_colormap_dim != color_colormap_dim:\n",
    "            resized_color_image = cv2.resize(color_image, dsize=(depth_colormap_dim[1], depth_colormap_dim[0]), interpolation=cv2.INTER_AREA)\n",
    "            images = np.hstack((resized_color_image, depth_colormap))\n",
    "        else:\n",
    "            images = np.hstack((color_image, depth_colormap))\n",
    "\n",
    "        # Show images\n",
    "        cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)\n",
    "        cv2.imshow('RealSense', images)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "finally:\n",
    "\n",
    "    # Stop streaming\n",
    "    pipeline.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9df0074-f174-462b-b473-732fef2cd372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo5_object_detection():\n",
    "    # Initialize YOLOv5 model (ensure the model file is available)\n",
    "    # model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # Load YOLOv5 small model\n",
    "    # model = Object_detector\n",
    "\n",
    "    # Configure RealSense pipeline\n",
    "    pipeline = rs.pipeline()\n",
    "    config = rs.config()\n",
    "    config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "    Object_colors = list(np.random.rand(80,3)*255)\n",
    "    # Start pipeline\n",
    "    pipeline.start(config)\n",
    "    cnt = 0\n",
    "    flag = 1\n",
    "    movement = 0.3\n",
    "    try:\n",
    "        while True:\n",
    "            \n",
    "            # Wait for a frame from the RealSense camera\n",
    "            frames = pipeline.wait_for_frames()\n",
    "            color_frame = frames.get_color_frame()\n",
    "\n",
    "            if not color_frame:\n",
    "                continue\n",
    "\n",
    "            # Convert RealSense frame to numpy array\n",
    "            color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "            # Use GStreamer (through OpenCV) to resize the image\n",
    "            frame = resized_image = cv2.resize(color_image, (640, 480))\n",
    "\n",
    "            # Perform YOLOv5 detection\n",
    "            # objs = results = model(resized_image)\n",
    "            objs = results = Object_detector.detect(resized_image)\n",
    "\n",
    "            for obj in objs:\n",
    "                print(\"=\"*5, obj)\n",
    "                print(\"done\")\n",
    "                label = obj[\"label\"]\n",
    "                score = obj[\"score\"]\n",
    "                \n",
    "                \n",
    "                print(\"1\")\n",
    "                [(xmin, ymin), (xmax, ymax)] = obj[\"bbox\"]\n",
    "                print(\"2\")\n",
    "                color = Object_colors[Object_classes.index(label)]\n",
    "                print(\"3\")\n",
    "                frame = cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "                print(\"4\")\n",
    "                frame = cv2.putText(frame,\n",
    "                        f\"{label} ({str(score )})\",\n",
    "                        (xmax, ymax),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.75,\n",
    "                        color,\n",
    "                        1,\n",
    "                        cv2.LINE_AA,\n",
    "                    )\n",
    "                print(f\"{label} ({str(score )})\")\n",
    "                if score > 0.40:\n",
    "                    time.sleep(.5)\n",
    "                    current_time = datetime.now()\n",
    "                    if label == 'chair' and (not time1 or ((current_time - time1) > 1)):\n",
    "                        time1 = datetime.strptime(\"12:30\", \"%H:%M\")\n",
    "                        # sport_client.StandUp()\n",
    "                        sport_client.Hello()\n",
    "                        flag = -1\n",
    "                        # sport_client.BalanceStand()\n",
    "                        # obsclient.UseRemoteCommandFromApi(True)\n",
    "                        # obsclient.Move(0,0,0.3)\n",
    "                        # time.sleep(1)\n",
    "                        # obsclient.Move(0,0,0)\n",
    "                        # obsclient.UseRemoteCommandFromApi(False)\n",
    "                        # movement = -movement\n",
    "                        # if flag == 1:\n",
    "                        #     print(\"standing up\")\n",
    "                        #     sport_client.StandUp()\n",
    "                        #     flag = -1\n",
    "                        # else:\n",
    "                        #     print(\"standing down\")\n",
    "                        #     sport_client.StandDown()\n",
    "                        #     flag = 1\n",
    "                    # elif label == 'bottle':\n",
    "                    #     robot.left(0.2)\n",
    "                    # elif label == 'tv':\n",
    "                    #     robot.backward(0.2)\n",
    "                    elif label == 'person':\n",
    "                        pass\n",
    "                        # sport_client.StandUp()\n",
    "                        # flag = -1\n",
    "                        # sport_client.BalanceStand()\n",
    "                        # sport_client.Move(movement,0,0)\n",
    "                        # movement = -movement\n",
    "                    \n",
    "            ipython_image = Image(data=cv2.imencode(\".jpeg\", frame)[1])  # Replace with your image file path\n",
    "\n",
    "            # Convert IPython Image to a format OpenCV can use\n",
    "            # 1. Get image bytes\n",
    "            image_bytes = ipython_image.data\n",
    "            \n",
    "            # 2. Convert bytes to a PIL image\n",
    "            pil_image = PILImage.open(io.BytesIO(image_bytes))\n",
    "            \n",
    "            # 3. Convert PIL image to a NumPy array\n",
    "            opencv_image = np.array(pil_image)\n",
    "            \n",
    "            # 4. Convert RGB to BGR (OpenCV uses BGR format)\n",
    "            opencv_image = cv2.cvtColor(opencv_image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Show the image using OpenCV\n",
    "            # cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)\n",
    "            # cv2.imshow(\"RealSense\", opencv_image)\n",
    "            cv2.imwrite(f\"{cnt}.jpg\", opencv_image)\n",
    "            cnt += 1\n",
    "\n",
    "            \n",
    "            \n",
    "            # cv2.imshow('RealSense', cv2.imencode(\".jpeg\", frame)[1])\n",
    "            # display(Image(data=cv2.imencode(\".jpeg\", frame)[1]))\n",
    "            clear_output(wait=True)\n",
    "            time.sleep(5)\n",
    "\n",
    "            # # Render the detection results\n",
    "            # detections = results.pandas().xyxy[0]  # Extract detections in Pandas format\n",
    "            # for _, row in detections.iterrows():\n",
    "            #     x1, y1, x2, y2, conf, cls = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax']), row['confidence'], row['name']\n",
    "            #     label = f\"{cls} {conf:.2f}\"\n",
    "            #     cv2.rectangle(resized_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            #     cv2.putText(resized_image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "            # # Display the resulting frame\n",
    "            # cv2.imshow('YOLOv5 Object Detection', resized_image)\n",
    "\n",
    "            # # Exit on pressing 'q'\n",
    "            # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            #     break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Stop the pipeline and release resources\n",
    "    \n",
    "        pipeline.stop()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "# Call the function to run the detection\n",
    "yolo5_object_detection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".jetbotenv",
   "language": "python",
   "name": ".jetbotenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
